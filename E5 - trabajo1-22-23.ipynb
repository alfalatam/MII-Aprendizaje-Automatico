{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48d50fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Aprendizaje automático \n",
    "# Máster en Ingeniería Informática - Universidad de Sevilla\n",
    "# Curso 2021-22\n",
    "# Primer trabajo práctico\n",
    "# ===========================================================\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# APELLIDOS: ALARCÓN TAMAYO\n",
    "# NOMBRE: ALFONSO\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# *****************************************************************************\n",
    "# HONESTIDAD ACADÉMICA Y COPIAS: un trabajo práctico es un examen, por lo que\n",
    "# debe realizarse de manera individual. La discusión y el intercambio de\n",
    "# información de carácter general con los compañeros se permite (e incluso se\n",
    "# recomienda), pero NO AL NIVEL DE CÓDIGO. Igualmente el remitir código de\n",
    "# terceros, OBTENIDO A TRAVÉS DE LA RED o cualquier otro medio, se considerará\n",
    "# plagio. \n",
    "\n",
    "# Cualquier plagio o compartición de código que se detecte significará\n",
    "# automáticamente la calificación de CERO EN LA ASIGNATURA para TODOS los\n",
    "# estudiantes involucrados. Por tanto, NO se les conservará, para\n",
    "# futuras convocatorias, ninguna nota que hubiesen obtenido hasta el\n",
    "# momento. SIN PERJUICIO DE OTRAS MEDIDAS DE CARÁCTER DISCIPLINARIO QUE SE\n",
    "# PUDIERAN TOMAR.  \n",
    "# *****************************************************************************\n",
    "\n",
    "\n",
    "# IMPORTANTE: NO CAMBIAR EL NOMBRE NI A ESTE ARCHIVO NI A LAS CLASES Y MÉTODOS\n",
    "# QUE SE PIDEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b23677f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================\n",
    "# IMPORTANTE: USO DE NUMPY\n",
    "# ========================\n",
    "\n",
    "# SE PIDE USAR NUMPY EN LA MEDIDA DE LO POSIBLE. \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# SE PENALIZARÁ el uso de bucles convencionales si la misma tarea se puede\n",
    "# hacer más eficiente con operaciones entre arrays que proporciona numpy. \n",
    "\n",
    "# PARTICULARMENTE IMPORTANTE es el uso del método numpy.dot. \n",
    "# Con numpy.dot podemos hacer productos escalares de pesos por características,\n",
    "# y extender esta operación de manera compacta a dos dimensiones, cuando tenemos \n",
    "# varias filas (ejemplos) e incluso varios varios vectores de pesos.  \n",
    "\n",
    "# En lo que sigue, los términos \"array\" o \"vector\" se refieren a \"arrays de numpy\".  \n",
    "\n",
    "# NOTA: En este trabajo NO se permite usar scikit-learn (salvo en el código que\n",
    "# se proporciona para cargar los datos).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81cde4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# *****************************************\n",
    "# CONJUNTOS DE DATOS A USAR EN ESTE TRABAJO\n",
    "# *****************************************\n",
    "\n",
    "# Para aplicar las implementaciones que se piden en este trabajo, vamos a usar\n",
    "# los siguientes conjuntos de datos. Para cargar todos los conjuntos de datos,\n",
    "# basta con descomprimir el archivo datos-trabajo-aa.zip y ejecutar el\n",
    "# archivo carga_datos.py (algunos de estos conjuntos de datos se cargan usando\n",
    "# utilidades de Scikit Learn). Todos los datos se cargan en arrays de numpy.\n",
    "\n",
    "# * Datos sobre concesión de prestamos en una entidad bancaria. En el propio\n",
    "#   archivo datos/credito.py se describe con más detalle. Se carga en las\n",
    "#   variables X_credito, y_credito.   \n",
    "\n",
    "# * Conjunto de datos de la planta del iris. Se carga en las variables X_iris,\n",
    "#   y_iris.  \n",
    "\n",
    "# * Datos sobre votos de cada uno de los 435 congresitas de Estados Unidos en\n",
    "#   17 votaciones realizadas durante 1984. Se trata de clasificar el partido al\n",
    "#   que pertenece un congresista (republicano o demócrata) en función de lo\n",
    "#   votado durante ese año. Se carga en las variables X_votos, y_votos. \n",
    "\n",
    "# * Datos de la Universidad de Wisconsin sobre posible imágenes de cáncer de\n",
    "#   mama, en función de una serie de características calculadas a partir de la\n",
    "#   imagen del tumor. Se carga en las variables X_cancer, y_cancer.\n",
    "  \n",
    "# * Críticas de cine en IMDB, clasificadas como positivas o negativas. El\n",
    "#   conjunto de datos que usaremos es sólo una parte de los textos. Los textos\n",
    "#   se han vectorizado usando CountVectorizer de Scikit Learn. Como vocabulario, \n",
    "#   se han usado las 609 palabras que ocurren más frecuentemente en las distintas \n",
    "#   críticas. Los datos se cargan finalmente en las variables X_train_imdb, \n",
    "#   X_test_imdb, y_train_imdb,y_test_imdb.    \n",
    "\n",
    "# * Un conjunto de imágenes (en formato texto), con una gran cantidad de\n",
    "#   dígitos (de 0 a 9) escritos a mano por diferentes personas, tomado de la\n",
    "#   base de datos MNIST. En digitdata.zip están todos los datos en formato\n",
    "#   comprimido. Para preparar estos datos habrá que escribir funciones que los\n",
    "#   extraigan de los ficheros de texto (más adelante se dan más detalles). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15d8acef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# CONJUNTOS DE DATOS A USAR EN EL PRIMER TRABAJO DE LA ASIGNATURA \n",
    "# \"APRENDIZAJE AUTOMÁTICO\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# CONCESIÓN DE UN PRÉSTAMO\n",
    "\n",
    "from datos import credito\n",
    "\n",
    "X_credito=np.array([d[:-1] for d in credito.datos_con_clas])\n",
    "y_credito=np.array([d[-1] for d in credito.datos_con_clas])\n",
    "\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# CLASIFICACIÓN DE LA PLANTA DE IRIS\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris=load_iris()\n",
    "X_iris=iris.data\n",
    "y_iris=iris.target\n",
    "\n",
    "\n",
    "# --------------------------------------------------\n",
    "\n",
    "# VOTOS EN EL CONGRESO USA\n",
    "\n",
    "from datos import votos\n",
    "X_votos=votos.datos\n",
    "y_votos=votos.clasif\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------\n",
    "\n",
    "# CÁNCER DE MAMA\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "cancer=load_breast_cancer()\n",
    "\n",
    "X_cancer=cancer.data\n",
    "y_cancer=cancer.target\n",
    "\n",
    "#-------------------------------------------\n",
    "\n",
    "# CRÍTICAS DE PELÍCULAS EN IMDB\n",
    "\n",
    "# Los datos están obtebidos de esta manera\n",
    "\n",
    "#import random as rd\n",
    "#from sklearn.datasets import load_files\n",
    "#\n",
    "#reviews_train = load_files(\"datos/aclImdb/train/\")\n",
    "#muestra_entr=rd.sample(list(zip(reviews_train.data,reviews_train.target)),k=2000)\n",
    "#text_train=[d[0] for d in muestra_entr]\n",
    "#text_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\n",
    "#y_train=np.array([d[1] for d in muestra_entr])\n",
    "#print(\"Ejemplos por cada clase: {}\".format(np.bincount(y_train)))\n",
    "#\n",
    "#reviews_test = load_files(\"datos/aclImdb/test/\")\n",
    "#muestra_test=rd.sample(list(zip(reviews_test.data,reviews_test.target)),k=400)\n",
    "#text_test=[d[0] for d in muestra_test]\n",
    "#text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\n",
    "#y_test=np.array([d[1] for d in muestra_test])\n",
    "#print(\"Ejemplos por cada clase: {}\".format(np.bincount(y_test)))\n",
    "#\n",
    "#\n",
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#\n",
    "#vect = CountVectorizer(min_df=50, stop_words=\"english\").fit(text_train)\n",
    "#print(\"Tamaño del vocabulario: {}\".format(len(vect.vocabulary_)))\n",
    "#X_train = vect.transform(text_train).toarray()\n",
    "#X_test = vect.transform(text_test).toarray()\n",
    "#\n",
    "#np.save(\"datos/imdb_sentiment/vect_train_text.npy\",X_train)\n",
    "#np.save(\"datos/imdb_sentiment/vect_test_text.npy\",X_test)\n",
    "#np.save(\"datos/imdb_sentiment/y_train_text.npy\",y_train)\n",
    "#np.save(\"datos/imdb_sentiment/y_test_text.npy\",y_test)\n",
    "\n",
    "X_train_imdb=np.load(\"datos/imdb_sentiment/vect_train_text.npy\")\n",
    "X_test_imdb=np.load(\"datos/imdb_sentiment/vect_test_text.npy\")\n",
    "y_train_imdb=np.load(\"datos/imdb_sentiment/y_train_text.npy\")\n",
    "y_test_imdb=np.load(\"datos/imdb_sentiment/y_test_text.npy\")\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# DÍGITOS ESCRITOS A MANO\n",
    "\n",
    "# ver digits.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d4ee964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================\n",
    "# EJERCICIO 1: SEPARACIÓN EN ENTRENAMIENTO Y PRUEBA (HOLDOUT)\n",
    "# ===========================================================\n",
    "\n",
    "# Definir una función \n",
    "\n",
    "#           particion_entr_prueba(X,y,test=0.20)\n",
    "\n",
    "# que recibiendo un conjunto de datos X, y sus correspondientes valores de\n",
    "# clasificación y, divide ambos en datos de entrenamiento y prueba, en la\n",
    "# proporción marcada por el argumento test, y conservando la correspondencia \n",
    "# original entre los ejemplos y sus valores de clasificación.\n",
    "# La división ha de ser ALEATORIA y ESTRATIFICADA respecto del valor de clasificación.\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Ejemplos:\n",
    "# =========\n",
    "\n",
    "# En votos:\n",
    "\n",
    "# In[1]: Xe_votos,Xp_votos,ye_votos,yp_votos          \n",
    "#            =particion_entr_prueba(X_votos,y_votos,test=1/3)\n",
    "\n",
    "# Como se observa, se han separado 2/3 para entrenamiento y 1/3 para prueba:\n",
    "# In[2]: y_votos.shape[0],ye_votos.shape[0],yp_votos.shape[0]\n",
    "# Out[2]: (435, 290, 145)\n",
    "\n",
    "# Las proporciones entre las clases son (aprox) las mismas en los dos conjuntos de\n",
    "# datos, y la misma que en el total: 267/168=178/112=89/56\n",
    "\n",
    "# In[3]: np.unique(y_votos,return_counts=True)\n",
    "# Out[3]: (array(['democrata', 'republicano'], dtype='<U11'), array([267, 168]))\n",
    "# In[4]: np.unique(ye_votos,return_counts=True)\n",
    "# Out[4]: (array(['democrata', 'republicano'], dtype='<U11'), array([178, 112]))\n",
    "# In[5]: np.unique(yp_votos,return_counts=True)\n",
    "# Out[5]: (array(['democrata', 'republicano'], dtype='<U11'), array([89, 56]))\n",
    "\n",
    "# La división en trozos es aleatoria y, por supuesto, en el orden en el que\n",
    "# aparecen los datos en Xe_votos,ye_votos y en Xp_votos,yp_votos, se preserva\n",
    "# la correspondencia original que hay en X_votos,y_votos.\n",
    "\n",
    "\n",
    "# Otro ejemplo con más de dos clases:\n",
    "\n",
    "# In[6]: Xe_credito,Xp_credito,ye_credito,yp_credito               \n",
    "#              =particion_entr_prueba(X_credito,y_credito,test=0.4)\n",
    "\n",
    "# In[7]: np.unique(y_credito,return_counts=True)\n",
    "# Out[7]: (array(['conceder', 'estudiar', 'no conceder'], dtype='<U11'),\n",
    "#          array([202, 228, 220]))\n",
    "\n",
    "# In[8]: np.unique(ye_credito,return_counts=True)\n",
    "# Out[8]: (array(['conceder', 'estudiar', 'no conceder'], dtype='<U11'),\n",
    "#          array([121, 137, 132]))\n",
    "\n",
    "# In[9]: np.unique(yp_credito,return_counts=True)\n",
    "# Out[9]: (array(['conceder', 'estudiar', 'no conceder'], dtype='<U11'),\n",
    "#          array([81, 91, 88]))\n",
    "# ------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee5f36",
   "metadata": {},
   "source": [
    "## <center> Ejercicio 1 </center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e8a34ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def particion_entr_prueba(X, y, test=0.20):\n",
    "    # Combina X e y en una matriz para mantener el orden\n",
    "    data = np.column_stack((X, y))\n",
    "    \n",
    "    # Barajo el conjunto para darle aleatoriedad\n",
    "    np.random.shuffle(data)\n",
    "    \n",
    "    # Ordenado en función de la última columna(valor de clasificación)\n",
    "    indices_ordenados = np.argsort(data[:, -1])\n",
    "\n",
    "    # Ordeno el conjunto\n",
    "    data_ordenada = data[indices_ordenados]\n",
    "    \n",
    "    # Calculo cuantos hay de cada tipo\n",
    "    clases_unicas, conteo_clases = np.unique(data_ordenada[:, -1], return_counts=True)\n",
    "    \n",
    "    # Creo los conjuntos en los que añadire los elementos\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "    \n",
    "    # Divide y añado los datos a los conjuntos de forma estratificada\n",
    "    for clase in clases_unicas:\n",
    "        # Filtro por clase\n",
    "        datos_clase = data_ordenada[data_ordenada[:, -1] == clase]\n",
    "        \n",
    "        # Calculo el indice donde debo cortar segun el tamaño especificado\n",
    "        idx_corte = int(len(datos_clase) * test)\n",
    "        \n",
    "        # Agrego los datos a entrenamiento\n",
    "        X_train.extend(datos_clase[idx_corte:, :-1])\n",
    "        y_train.extend(datos_clase[idx_corte:, -1])\n",
    "        \n",
    "        # Agrego los datos a prueba\n",
    "        X_test.extend(datos_clase[:idx_corte, :-1])\n",
    "        y_test.extend(datos_clase[:idx_corte, -1])\n",
    "        \n",
    "    #Para que no esten \"ordenados\" los vuelvo a reordenar juntando los conjuntos y barajandoslos como al principio\n",
    "    train = np.column_stack((X_train, y_train))\n",
    "    np.random.shuffle(train)\n",
    "    \n",
    "    # Finalmente los separo en caractertisticas y valor de clasificacion\n",
    "    X_train = train[:, :-1]\n",
    "    y_train = train[:, -1]\n",
    "    \n",
    "    #repito lo mismo que antes pero para test\n",
    "    test = np.column_stack((X_test, y_test))\n",
    "    np.random.shuffle(test)\n",
    "    \n",
    "    #vuevlo a separar los conjuntos\n",
    "    X_test = test[:, :-1]\n",
    "    y_test = test[:, -1]\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(X_train), np.array(X_test), np.array(y_train), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fe2a44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso:\n",
    "X_train, X_test, y_train,y_test =particion_entr_prueba(X_votos,y_votos,test=1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f4db4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "# EJERCICIO 2: REGRESIÓN LOGÍSTICA MINI-BATCH\n",
    "# ===========================================\n",
    "\n",
    "\n",
    "# Se pide implementar el clasificador de regresión logística mini-batch \n",
    "# a través de una clase python, que ha de tener la siguiente estructura:\n",
    "\n",
    "# class RegresionLogisticaMiniBatch():\n",
    "\n",
    "#    def __init__(self,normalizacion=False,\n",
    "#                 rate=0.1,rate_decay=False,batch_tam=64,\n",
    "#                 pesos_iniciales=None):\n",
    "\n",
    "#          .....\n",
    "         \n",
    "#    def entrena(self,entr,clas_entr,n_epochs=1000,\n",
    "#                reiniciar_pesos=False):\n",
    "\n",
    "#         ......\n",
    "\n",
    "#     def clasifica_prob(self,E):\n",
    "\n",
    "\n",
    "#         ......\n",
    "\n",
    "#     def clasifica(self,E):\n",
    "\n",
    "\n",
    "#         ......\n",
    "        \n",
    "\n",
    "# Explicamos a continuación cada uno de los métodos:\n",
    "\n",
    "\n",
    "# * Constructor de la clase:\n",
    "# --------------------------\n",
    "\n",
    "#  El constructor debe tener los siguientes argumentos de entrada:\n",
    "\n",
    "\n",
    "#  - El parámetro normalizacion, que puede ser True o False (False por\n",
    "#    defecto). Indica si los datos se tienen que normalizar, tanto para el\n",
    "#    entrenamiento como para la clasificación de nuevas instancias.  La\n",
    "#    normalización es una técnica que suele ser útil cuando los distintos\n",
    "#    atributos reflejan cantidades numéricas de muy distinta magnitud.\n",
    "#    En ese caso, antes de entrenar se calcula la media m_i y la desviación\n",
    "#    típica d_i en CADA COLUMNA i (es decir, en cada atributo) de los\n",
    "#    datos del conjunto de entrenamiento.  A continuación, y antes del\n",
    "#    entrenamiento, esos datos se transforman de manera que cada componente\n",
    "#    x_i se cambia por (x_i - m_i)/d_i. Esta MISMA transformación se realiza\n",
    "#    sobre las nuevas instancias que se quieran clasificar.\n",
    "\n",
    "#  - rate: si rate_decay es False, rate es la tasa de aprendizaje fija usada\n",
    "#    durante todo el aprendizaje. Si rate_decay es True, rate es la\n",
    "#    tasa de aprendizaje inicial. Su valor por defecto es 0.1.\n",
    "\n",
    "#  - rate_decay, indica si la tasa de aprendizaje debe disminuir en\n",
    "#    cada epoch. En concreto, si rate_decay es True, la tasa de\n",
    "#    aprendizaje que se usa en el n-ésimo epoch se debe de calcular\n",
    "#    con la siguiente fórmula: \n",
    "#       rate_n= (rate_0)*(1/(1+n)) \n",
    "#    donde n es el número de epoch, y rate_0 es la cantidad\n",
    "#    introducida en el parámetro rate anterior.   \n",
    "\n",
    "#  - batch_tam: indica el tamaño de los mini batches (por defecto 64)\n",
    "#    que se usan para calcular cada actualización de pesos.\n",
    "    \n",
    "#  - pesos_iniciales: Si es None, los pesos iniciales se inician \n",
    "#    aleatoriamente. Si no, debe proporcionar un array de pesos que se \n",
    "#    tomarán como pesos iniciales.     \n",
    "\n",
    "# \n",
    "\n",
    "# * Método entrena:\n",
    "# -----------------\n",
    "\n",
    "#  Este método es el que realiza el entrenamiento del clasificador. \n",
    "#  Debe calcular un vector de pesos, mediante el correspondiente\n",
    "#  algoritmo de entrenamiento basado en ascenso por el gradiente mini-batch, \n",
    "#  para maximizar la log verosimilitud. Describimos a continuación los parámetros de\n",
    "#  entrada:  \n",
    "\n",
    "#  - entr y clas_entr, son los datos del conjunto de entrenamiento y su\n",
    "#    clasificación, respectivamente. El primero es un array (bidimensional)  \n",
    "#    con los ejemplos, y el segundo un array (unidimensional) con las clasificaciones \n",
    "#    de esos ejemplos, en el mismo orden. \n",
    "\n",
    "#  - n_epochs: número de pasadas que se realizan sobre todo el conjunto de\n",
    "#    entrenamiento.\n",
    "\n",
    "#  - reiniciar_pesos: si es True, se reinicia al comienzo del \n",
    "#    entrenamiento el vector de pesos de manera aleatoria \n",
    "#    (típicamente, valores aleatorios entre -1 y 1).\n",
    "#    Si es False, solo se inician los pesos la primera vez que se\n",
    "#    llama a entrena. En posteriores veces, se parte del vector de\n",
    "#    pesos calculado en el entrenamiento anterior. Esto puede ser útil\n",
    "#    para continuar el aprendizaje a partir de un aprendizaje\n",
    "#    anterior, si por ejemplo se dispone de nuevos datos.     \n",
    "\n",
    "#  NOTA: El entrenamiento en mini-batch supone que en cada epoch se\n",
    "#  recorren todos los ejemplos del conjunto de entrenamiento,\n",
    "#  agrupados en grupos del tamaño indicado. Por cada uno de estos\n",
    "#  grupos de ejemplos se produce una actualización de los pesos. \n",
    "#  Se pide una VERSIÓN ESTOCÁSTICA, en la que en cada epoch se asegura que \n",
    "#  se recorren todos los ejemplos del conjunto de entrenamiento, \n",
    "#  en un orden ALEATORIO, aunque agrupados en grupos del tamaño indicado. \n",
    "\n",
    "\n",
    "# * Método clasifica_prob:\n",
    "# ------------------------\n",
    "\n",
    "#  Método que devuelve el array de correspondientes probabilidades de pertenecer \n",
    "#  a la clase positiva (la que se ha tomado como clase 1), para cada ejemplo de un \n",
    "#  array E de nuevos ejemplos.\n",
    "\n",
    "\n",
    "        \n",
    "# * Método clasifica:\n",
    "# -------------------\n",
    "    \n",
    "#  Método que devuelve un array con las correspondientes clases que se predicen\n",
    "#  para cada ejemplo de un array E de nuevos ejemplos. La clase debe ser una de las \n",
    "#  clases originales del problema (por ejemplo, \"republicano\" o \"democrata\" en el \n",
    "#  problema de los votos).  \n",
    "\n",
    "\n",
    "# Si el clasificador aún no ha sido entrenado, tanto \"clasifica\" como\n",
    "# \"clasifica_prob\" deben devolver una excepción del siguiente tipo:\n",
    "\n",
    "class ClasificadorNoEntrenado(Exception): pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ejemplos de uso:\n",
    "# ----------------\n",
    "\n",
    "\n",
    "\n",
    "# CON LOS DATOS VOTOS:\n",
    "        \n",
    "#   \n",
    "\n",
    "# En primer lugar, separamos los datos en entrenamiento y prueba (los resultados pueden\n",
    "# cambiar, ya que esta partición es aleatoria)\n",
    "\n",
    "        \n",
    "# In [1]: Xe_votos,Xp_votos,ye_votos,yp_votos            \n",
    "#            =particion_entr_prueba(X_votos,y_votos)\n",
    "\n",
    "# Creamos el clasificador:\n",
    "        \n",
    "# In [2]: RLMB_votos=RegresionLogisticaMiniBatch()\n",
    "\n",
    "# Lo entrenamos sobre los datos de entrenamiento:\n",
    "\n",
    "# In [3]: RLMB_votos.entrena(Xe_votos,ye_votos)\n",
    "\n",
    "# Con el clasificador aprendido, realizamos la predicción de las clases\n",
    "# de los datos que estan en test:\n",
    "\n",
    "# In [4]: RLMB_votos.clasifica_prob(Xp_votos)\n",
    "# array([3.90234132e-04, 1.48717603e-11, 3.90234132e-04, 9.99994374e-01, 9.99347533e-01,...]) \n",
    "        \n",
    "# In [5]: RLMB_votos.clasifica(Xp_votos)\n",
    "# Out[5]: array(['democrata', 'democrata', 'democrata','republicano',... ], dtype='<U11')\n",
    "\n",
    "# Calculamos la proporción de aciertos en la predicción, usando la siguiente \n",
    "# función que llamaremos \"rendimiento\".\n",
    "\n",
    "def rendimiento(clasif,X,y):\n",
    "    return sum(clasif.clasifica(X)==y)/y.shape[0]\n",
    "        \n",
    "# In [6]: rendimiento(RLMB_votos,Xp_votos,yp_votos)\n",
    "# Out[6]: 0.9080459770114943    \n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# CON LOS DATOS DEL CÀNCER\n",
    "        \n",
    "# Hacemos un experimento similar al anterior, pero ahora con los datos del \n",
    "# cáncer de mama, y usando normalización y disminución de la tasa         \n",
    "\n",
    "# In[7]: Xe_cancer,Xp_cancer,ye_cancer,yp_cancer           \n",
    "#           =particion_entr_prueba(X_cancer,y_cancer)\n",
    "\n",
    "\n",
    "# In[8]: RLMB_cancer=RegresionLogisticaMiniBatch(normalizacion=True,rate_decay=True)\n",
    "\n",
    "# In[9]: RLMB_cancer.entrena(Xe_cancer,ye_cancer)\n",
    "\n",
    "# In[9]: RLMB_cancer.clasifica_prob(Xp_cancer)\n",
    "# Out[9]: array([9.85046885e-01, 8.77579844e-01, 7.81826115e-07,..])\n",
    "\n",
    "# In[10]: RLMB_cancer.clasifica(Xp_cancer)\n",
    "# Out[10]: array([1, 1, 0,...])\n",
    "\n",
    "# In[11]: rendimiento(RLMB_cancer,Xp_cancer,yp_cancer)\n",
    "# Out[11]: 0.9557522123893806\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da73cb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9dde69b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegresionLogisticaMiniBatch():\n",
    "\n",
    "    def __init__(self,normalizacion=False,rate=0.1,rate_decay=False,batch_tam=64,pesos_iniciales=None):\n",
    "        # False por defecto \n",
    "        self.normalizacion = normalizacion\n",
    "        self.rate = rate\n",
    "        self.rate_decay=rate_decay\n",
    "        self.batch_tam = batch_tam\n",
    "        self.pesos_iniciales = pesos_iniciales\n",
    "        \n",
    "        # los pesos y el bias los defininos de base a None\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def normaliza(self,X):\n",
    "            \n",
    "        # Como vamos a realizar los cambios en las características,definimos axis=0 indicando que lo realice para las columnas\n",
    "        medias = np.mean(X, axis=0)\n",
    "        desviaciones = np.std(X, axis=0)\n",
    "        # Calculadas las medias y desviaciones simplemente aplicamos (x_i - m_i)/d_i.\n",
    "        return (X - medias) / desviaciones\n",
    "\n",
    "    \n",
    "    def entrena(self,entr,clas_entr,n_epochs=1000,reiniciar_pesos=False):\n",
    "        \n",
    "        # Si hace falta normalizar\n",
    "        if(self.normalizacion == True):\n",
    "            entr = self.normaliza(entr)\n",
    "        \n",
    "        # para saber la dimensión del conjunto y poder generar los pesos\n",
    "        m,n = entr.shape\n",
    "        \n",
    "        if(reiniciar_pesos == True or self.pesos_iniciales == None):\n",
    "            \n",
    "            # Iniciar los pesos entre -1 y 1\n",
    "            self.weights = np.random.uniform(-1, 1, size=n)\n",
    "            self.bias = 0 \n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            \n",
    "            # Aqui actualizamos el rate decay o si no se actualiza\n",
    "            if self.rate_decay == True:\n",
    "                rate_n = self.rate / (1 + epoch)\n",
    "            else:\n",
    "                rate_n = self.rate\n",
    "            \n",
    "            # añadimos aleatoriedad\n",
    "            indices_perm = np.random.permutation(m)\n",
    "            entr = entr[indices_perm]\n",
    "            clas_entr = clas_entr[indices_perm]\n",
    "            \n",
    "            # aqui se hace el mini batch, vamos a 0 al nº de filas de el nº de mini_batch que es 64 por defecto\n",
    "            for i in range(0, m, self.batch_tam):\n",
    "                # ahora vamos de batch_tam haciendo la actualizacion,seleccionamos los x elementos\n",
    "                entradas_mb = entr[i:self.batch_tam + i]\n",
    "                clas_entr_mb = clas_entr[i: self.batch_tam + i]\n",
    "                \n",
    "                # aqui hacemos las actualizaciones\n",
    "                # Del las batch_size entradas multiplicamos cada una por el peso y le sumanos la bias\n",
    "                z = np.dot(entradas_mb, self.weights) + self.bias\n",
    "                \n",
    "                \n",
    "                # con la funcion sigmoide obtneemos la prob de que pertenezca a la clase positiva\n",
    "                activacion = 1 / (1 + np.exp(- z))\n",
    "\n",
    "                # compraramos el error/diferencia del resultado obtenido con respecto a los valores de clasificación \n",
    "                # queremos saber cuanto nos hemos equivocado\n",
    "                \n",
    "                # calculamos el error\n",
    "                error =  clas_entr_mb - activacion\n",
    "                \n",
    "                # calculamos la derivada parcial y el gradiente multiplcando la traspuesta por el error obtenido y dividiendo por este caso por el batch\n",
    "                gradiente_weights = np.dot(entradas_mb.T, error) / self.batch_tam\n",
    "                \n",
    "                # para actualizar los pesos\n",
    "                gradiente_bias = np.sum(error) / self.batch_tam\n",
    "\n",
    "                # Actualizamos los pesos  y el bias, la actualizacion de estos valores nos indicarán la dirección\n",
    "                # al calcular el gradiente este nos indica la pendiente, en el ascenso vamos en la dirección de esta,para el descenso\n",
    "                # deberiamos ir en la contraría, por lo tanto restar\n",
    "\n",
    "                self.weights = self.weights + rate_n * gradiente_weights\n",
    "                self.bias = self.bias + rate_n * gradiente_bias\n",
    "\n",
    "                \n",
    "                \n",
    "\n",
    "\n",
    "    def clasifica_prob(self,E):\n",
    "        # Comprobamos si hemos añadido pesos, si no lanza excepcion\n",
    "        if(self.weights is None):\n",
    "            raise ClasificadorNoEntrenado(\"Clasificador no entrenado\")\n",
    "\n",
    "        # normalizar\n",
    "        if(self.normalizacion == True ):\n",
    "            E = self.normaliza(E)\n",
    "        #\n",
    "        z = np.dot(E, self.weights) + self.bias\n",
    "        activacion = 1 / (1 + np.exp(-z))\n",
    "        return activacion\n",
    "\n",
    "\n",
    "\n",
    "    def clasifica(self,E):\n",
    "        if(self.weights is None):\n",
    "            raise ClasificadorNoEntrenado(\"Clasificador no entrenado\")\n",
    "        # Aqui sacamos los datos de pertenecer a la clase de clasifica_prob y con numpy,ponemos un umbral de 0,5.Si es mayor\n",
    "        # lo clasificará como clase positiva si no es el caso como negativa\n",
    "        prob = self.clasifica_prob(E)\n",
    "        return np.where(prob >= 0.5, 1, 0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b488e3",
   "metadata": {},
   "source": [
    "En el caso de los votos, los valores de clasificación es si pertenece al partido democrata o republicano, por lo que vamos a tener que mapear estos valores a 0 y 1  antes de llamar a la clasificación "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f857fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definir un diccionario de mapeo de etiquetas a valores binarios\n",
    "label_mapping = {\"republicano\": 0, \"democrata\": 1}\n",
    "\n",
    "# Aplicar el mapeo a cada elemento en y_test\n",
    "y_test_encoded = [label_mapping[label] for label in y_votos]\n",
    "\n",
    "Xe_votos,Xp_votos,ye_votos,yp_votos=particion_entr_prueba(X_votos,y_test_encoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dbb8f67c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9418604651162791"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el clasificador:\n",
    "        \n",
    "RLMB_votos=RegresionLogisticaMiniBatch()\n",
    "\n",
    "# Lo entrenamos sobre los datos de entrenamiento:\n",
    "\n",
    "RLMB_votos.entrena(Xe_votos,ye_votos)\n",
    "\n",
    "        \n",
    "rendimiento(RLMB_votos,Xp_votos,yp_votos)\n",
    "# Out[6]: 0.9080459770114943    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13bcd2a",
   "metadata": {},
   "source": [
    "Ahora probamos con el cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7526024b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9469026548672567"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xe_cancer,Xp_cancer,ye_cancer,yp_cancer=particion_entr_prueba(X_cancer,y_cancer)\n",
    "\n",
    "RLMB_cancer=RegresionLogisticaMiniBatch(normalizacion=True,rate_decay=True)\n",
    "\n",
    "RLMB_cancer.entrena(Xe_cancer,ye_cancer)\n",
    "\n",
    "RLMB_cancer.clasifica_prob(Xp_cancer)\n",
    "# Out[9]: array([9.85046885e-01, 8.77579844e-01, 7.81826115e-07,..])\n",
    "\n",
    "RLMB_cancer.clasifica(Xp_cancer)\n",
    "# Out[10]: array([1, 1, 0,...])\n",
    "\n",
    "rendimiento(RLMB_cancer,Xp_cancer,yp_cancer)\n",
    "# Out[11]: 0.9557522123893806"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd543c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ec6baca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================\n",
    "# EJERCICIO 3: IMPLEMENTACIÓN DE VALIDACIÓN CRUZADA\n",
    "# =================================================\n",
    "\n",
    "# Este ejercicio vale 2 PUNTOS (SOBRE 10) pero se puede saltar, sin afectar \n",
    "# al resto del trabajo. Puede servir para el ajuste de parámetros en los ejercicios \n",
    "# posteriores, pero si no se realiza, se podrían ajustar siguiendo el método \"holdout\" \n",
    "# implementado en el ejercicio 1. \n",
    "\n",
    "# La técnica de validación cruzada que se pide en este ejercicio se explica\n",
    "# en el tema \"Evaluación de modelos\".     \n",
    "\n",
    "# Definir una función: \n",
    "\n",
    "#  rendimiento_validacion_cruzada(clase_clasificador,params,X,y,n=5)\n",
    "\n",
    "# que devuelve el rendimiento medio de un clasificador, mediante la técnica de\n",
    "# validación cruzada con n particiones. Los arrays X e y son los datos y la\n",
    "# clasificación esperada, respectivamente. El argumento clase_clasificador es\n",
    "# el nombre de la clase que implementa el clasificador. El argumento params es\n",
    "# un diccionario cuyas claves son nombres de parámetros del constructor del\n",
    "# clasificador y los valores asociados a esas claves son los valores de esos\n",
    "# parámetros para llamar al constructor.\n",
    "\n",
    "# INDICACIÓN: para usar params al llamar al constructor del clasificador, usar\n",
    "# clase_clasificador(**params)  \n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Ejemplo:\n",
    "# --------\n",
    "# Lo que sigue es un ejemplo de cómo podríamos usar esta función para\n",
    "# ajustar el valor de algún parámetro. En este caso aplicamos validación\n",
    "# cruzada, con n=5, en el conjunto de datos del cáncer, para estimar cómo de\n",
    "# bueno es el valor batch_tam=16 con rate_decay en regresión logística mini_batch.\n",
    "# Usando la función que se pide sería (nótese que debido a la aleatoriedad, \n",
    "# no tiene por qué coincidir exactamente el resultado):\n",
    "\n",
    "# >>> rendimiento_validacion_cruzada(RegresionLogisticaMiniBatch,         \n",
    "#             {\"batch_tam\":16,\"rate_decay\":True},Xe_cancer,ye_cancer,n=5)\n",
    "# 0.9121095227289917\n",
    "\n",
    "\n",
    "# El resultado es la media de rendimientos obtenidos entrenando cada vez con\n",
    "# todas las particiones menos una, y probando el rendimiento con la parte que\n",
    "# se ha dejado fuera. Las particiones deben ser aleatorias y estratificadas. \n",
    " \n",
    "# Si decidimos que es es un buen rendimiento (comparando con lo obtenido para\n",
    "# otros valores de esos parámetros), finalmente entrenaríamos con el conjunto de\n",
    "# entrenamiento completo:\n",
    "\n",
    "# >>> LR16=RegresionLogisticaMiniBatch(batch_tam=16,rate_decay=True)\n",
    "# >>> LR16.entrena(Xe_cancer,ye_cancer)\n",
    "\n",
    "# Y daríamos como estimación final el rendimiento en el conjunto de prueba, que\n",
    "# hasta ahora no hemos usado:\n",
    "# >>> rendimiento(LR16,Xp_cancer,yp_cancer)\n",
    "# 0.9203539823008849\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6475e32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rendimiento_validacion_cruzada(clase_clasificador, params, X, y, n=5):\n",
    "    \n",
    "    # Crear n particiones aleatorias y estratificadas\n",
    "    indices = np.arange(len(y))\n",
    "    np.random.shuffle(indices)\n",
    "    particiones = np.array_split(indices, n)    \n",
    "    \n",
    "    # creo una lista donde guardar  los rendimientos\n",
    "    rendimientos = []\n",
    "    \n",
    "    # Recorro las n partes \n",
    "    for i in range(n):\n",
    "    \n",
    "        # crear x_test y y_test \n",
    "        indices_prueba = particiones[i]\n",
    "        X_test, y_test =  X[indices_prueba], y[indices_prueba]\n",
    "        \n",
    "        # Eliminamos la parte del conjunto de prueba\n",
    "        particiones.pop(i)\n",
    "\n",
    "        \n",
    "        # crear x_train y y_train\n",
    "        # concatenar los otros 4 trozos en el conjunto de entrenamiento\n",
    "        indices_entrenamiento = np.concatenate(particiones)\n",
    "        X_train, y_train = X[indices_entrenamiento],y[indices_entrenamiento]\n",
    "        \n",
    "        # añadimos el elemento eliminado\n",
    "        particiones.insert(i, indices_prueba)\n",
    "        \n",
    "        # creamos y entrenamos el clasificador\n",
    "        clasificador = clase_clasificador(**params)\n",
    "        clasificador.entrena(X_train, y_train)\n",
    "\n",
    "        # calcular el rendimiento\n",
    "        rendimiento_prueba = rendimiento(clasificador, X_test, y_test)\n",
    "        \n",
    "        # guardamos en la lista el resultado\n",
    "        rendimientos.append(rendimiento_prueba)\n",
    "\n",
    "    # Devolver la lista de rendimientos\n",
    "    return np.mean(rendimientos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bab00b10",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alarc\\AppData\\Local\\Temp\\ipykernel_23316\\4125834256.py:64: RuntimeWarning: overflow encountered in exp\n",
      "  activacion = 1 / (1 + np.exp(- z))\n",
      "C:\\Users\\alarc\\AppData\\Local\\Temp\\ipykernel_23316\\4125834256.py:99: RuntimeWarning: overflow encountered in exp\n",
      "  activacion = 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9014094601051124"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rendimiento_validacion_cruzada(RegresionLogisticaMiniBatch, {\"batch_tam\":32,\"rate_decay\":True},Xe_cancer,ye_cancer,n=5)\n",
    "# En el ejemplo se espera un 0.9121095227289917"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cea72fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# EJERCICIO 4: APLICANDO LOS CLASIFICADORES BINARIOS\n",
    "# ===================================================\n",
    "\n",
    "\n",
    "\n",
    "# Usando los dos modelos implementados en el ejercicio 3, obtener clasificadores \n",
    "# con el mejor rendimiento posible para los siguientes conjunto de datos:\n",
    "\n",
    "# - Votos de congresistas US\n",
    "# - Cáncer de mama \n",
    "# - Críticas de películas en IMDB\n",
    "\n",
    "# Ajustar los parámetros para mejorar el rendimiento. Si se ha hecho el ejercicio 3, \n",
    "# usar validación cruzada para el ajuste (si no, usar el \"holdout\" del ejercicio 1). \n",
    "\n",
    "# Mostrar el proceso realizado en cada caso, y los rendimientos finales obtenidos. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9236d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculaMejoresClasificadores(X, y, param_list):\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "\n",
    "    # Recorremos sobre los parámetros\n",
    "    for params in param_list:\n",
    "        # Calculamos el rendimiento mediante validación cruzada\n",
    "        rend_vc = rendimiento_validacion_cruzada(RegresionLogisticaMiniBatch, params, X, y, n=5)\n",
    "\n",
    "        # Se crea y entrena un modelo con los parámetros \n",
    "        modelo = RegresionLogisticaMiniBatch(**params)\n",
    "        modelo.entrena(X, y)\n",
    "\n",
    "        # Calcular el rendimiento del modelo entrenado\n",
    "        rend_modelo = rendimiento(modelo, X, y)\n",
    "\n",
    "        # Tomar el máximo rendimiento entre validación cruzada y rendimiento del modelo directo\n",
    "        max_score = max(rend_vc, rend_modelo)\n",
    "\n",
    "        # Actualizar los mejores parámetros y el mejor rendimiento si es necesario\n",
    "        if max_score > best_score:\n",
    "            best_score = max_score\n",
    "            best_params = params\n",
    "\n",
    "    # Si se quiere mostrar los parametros         \n",
    "    # Mostrar los mejores parámetros y rendimiento\n",
    "    print(\"Mejores parámetros:\", best_params)\n",
    "    print(\"Mejor rendimiento:\", best_score)\n",
    "\n",
    "    # Devolver el mejor modelo\n",
    "    best_model = RegresionLogisticaMiniBatch(**best_params)\n",
    "    best_model.entrena(X, y)\n",
    "    return best_model\n",
    "\n",
    "#Listado de parametros (podrian ser más pero el tiempo de ejecución es mi ordenador crece exponencialmente y con estos ya tarda)\n",
    "params_list = [\n",
    "    {\"rate_decay\": True, \"batch_tam\": 16, \"normalizacion\": True, \"rate\": 0.001},\n",
    "    {\"rate_decay\": False, \"batch_tam\": 32, \"normalizacion\": False, \"rate\": 0.1},\n",
    "    {\"rate_decay\": False, \"batch_tam\": 64, \"normalizacion\": False, \"rate\": 0.01},\n",
    "    {\"rate_decay\": False, \"batch_tam\": 32, \"normalizacion\": False, \"rate\": 0.001},\n",
    "    {\"rate_decay\": False, \"batch_tam\": 16, \"normalizacion\": True, \"rate\": 0.01},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f2a403",
   "metadata": {},
   "source": [
    "Calculamos el mejor modelo para el conjunto de datos de Cancer, votos y imdb respectivamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbcb9864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alarc\\AppData\\Local\\Temp\\ipykernel_23316\\4125834256.py:64: RuntimeWarning: overflow encountered in exp\n",
      "  activacion = 1 / (1 + np.exp(- z))\n",
      "C:\\Users\\alarc\\AppData\\Local\\Temp\\ipykernel_23316\\4125834256.py:99: RuntimeWarning: overflow encountered in exp\n",
      "  activacion = 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'rate_decay': False, 'batch_tam': 16, 'normalizacion': True, 'rate': 0.01}\n",
      "Mejor rendimiento: 0.9876977152899824\n"
     ]
    }
   ],
   "source": [
    "mejor_modelo_cancer = calculaMejoresClasificadores(X_cancer, y_cancer, params_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c18999e",
   "metadata": {},
   "source": [
    "Como comentamos tenemos que mapear los valores de clasificación de este conjunto, pues no son 0 y 1. Esto ya lo hicimos para el apartado 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c5704b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'rate_decay': False, 'batch_tam': 32, 'normalizacion': False, 'rate': 0.1}\n",
      "Mejor rendimiento: 0.9656160458452722\n"
     ]
    }
   ],
   "source": [
    "# Hacemos un mapeo de etiquetas a valores binarios\n",
    "label_mapping = {\"republicano\": 0, \"democrata\": 1}\n",
    "\n",
    "# Aplicar el mapeo a cada elemento en y_test\n",
    "y_test = [label_mapping[label] for label in y_votos]\n",
    "\n",
    "Xe_congr,Xp_congr,ye_congr,yp_congr=particion_entr_prueba(X_votos,y_test)\n",
    "\n",
    "mejor_modelo_congresistas = calculaMejoresClasificadores(Xe_congr, ye_congr, params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f01f5868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'rate_decay': False, 'batch_tam': 16, 'normalizacion': True, 'rate': 0.01}\n",
      "Mejor rendimiento: 0.983\n"
     ]
    }
   ],
   "source": [
    "mejor_modelo_imdb = calculaMejoresClasificadores(X_train_imdb, y_train_imdb, params_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7c348e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# EJERCICIO 5: CLASIFICACIÓN MULTICLASE\n",
    "# =====================================\n",
    "\n",
    "# Técnica \"One vs Rest\" (Uno frente al Resto)\n",
    "# -------------------------------------------\n",
    "\n",
    "\n",
    "# Se pide implementar la técnica \"One vs Rest\" (Uno frente al Resto),\n",
    "# para obtener un clasificador multiclase a partir del clasificador\n",
    "# binario definido en el apartado anterior.\n",
    "\n",
    "\n",
    "#  En concreto, se pide implementar una clase python\n",
    "#  RegresionLogisticaOvR con la siguiente estructura, y que implemente\n",
    "#  el entrenamiento y la clasificación siguiendo el método \"One vs\n",
    "#  Rest\" tal y como se ha explicado en las diapositivas del módulo.\n",
    "\n",
    " \n",
    "\n",
    "# class RegresionLogisticaOvR():\n",
    "\n",
    "#    def __init__(self,normalizacion=False,rate=0.1,rate_decay=False,\n",
    "#                 batch_tam=64):\n",
    "\n",
    "#          .....\n",
    "         \n",
    "#    def entrena(self,entr,clas_entr,n_epochs=1000):\n",
    "\n",
    "#         ......\n",
    "\n",
    "#    def clasifica(self,E):\n",
    "\n",
    "\n",
    "#         ......\n",
    "        \n",
    "\n",
    "\n",
    "#  Los parámetros de los métodos significan lo mismo que en el\n",
    "#  apartado anterior.\n",
    "\n",
    "#  Un ejemplo de sesión, con el problema del iris:\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# In[1] Xe_iris,Xp_iris,ye_iris,yp_iris          \n",
    "#            =particion_entr_prueba(X_iris,y_iris,test=1/3)\n",
    "\n",
    "# >>> rl_iris=RL_OvR(rate=0.001,batch_tam=20)\n",
    "\n",
    "# >>> rl_iris.entrena(Xe_iris,ye_iris)\n",
    "\n",
    "# >>> rendimiento(rl_iris,Xe_iris,ye_iris)\n",
    "# 0.9797979797979798\n",
    "\n",
    "# >>> rendimiento(rl_iris,Xp_iris,yp_iris)\n",
    "# >>> 0.9607843137254902\n",
    "# --------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d583790",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegresionLogisticaOvR():\n",
    "\n",
    "    def __init__(self,normalizacion=False,rate=0.1,rate_decay=False,batch_tam=64):\n",
    "        \n",
    "        self.normalizacion = normalizacion\n",
    "        self.rate = rate\n",
    "        self.rate_decay=rate_decay\n",
    "        self.batch_tam = batch_tam\n",
    "        \n",
    "        self.classifiers = []\n",
    "        self.clasificador = {}\n",
    "        \n",
    "             \n",
    "    def entrena(self,entr,clas_entr,n_epochs=1000):\n",
    "        \n",
    "        # Calculamos los elementos\n",
    "        clases_unicas = np.unique(clas_entr).astype(int)\n",
    "        # Tengo que hacer un claisficador binario por cada clase unica\n",
    "        # en caso de ser A,B o C, pruebo A con respecto(B y C) y asi con los 3 casos\n",
    "        \n",
    "        for i in clases_unicas:\n",
    "            \n",
    "            # Convierto esos valores de clasificación a binario usando a clases_unicas[i] como la positiva\n",
    "            # Crear una nueva matriz de etiquetas binarias\n",
    "            clas_entr_binario = (clas_entr == i).astype(int)\n",
    "            \n",
    "            # Llamamos al clasificador quew hemos creado en el apartado anterior\n",
    "            mejor_modelo_binario = calculaMejoresClasificadores(entr, clas_entr_binario, params_list)\n",
    "            # en el listado de clasificadores, vamos añadiendo los binarios\n",
    "            self.classifiers.append(mejor_modelo_binario)\n",
    "            \n",
    "            self.clasificador[clases_unicas[i]] = mejor_modelo_binario\n",
    "                       \n",
    "\n",
    "    def clasifica(self,E):\n",
    "        \n",
    "        # crear un array del tamaño de filas de E vacio\n",
    "        y = np.zeros_like(E)\n",
    "        \n",
    "        #Aqui almacenare las predicciones\n",
    "        a=[]\n",
    "        \n",
    "        # Recorremos los modelos\n",
    "        for model in self.clasificador.values():        \n",
    "            a.append(model.clasifica_prob(E))\n",
    "        # Creamos un array multidimensional               \n",
    "        array_multidimensional = np.stack(a)\n",
    "\n",
    "        #Seleccionamos la clase con la probabilidad máxima para cada fila\n",
    "        # hacemos la traspuesta para recorrer los diferentes clasificaciones y nos quedamos la mayor clasificación y devolvemos\n",
    "        # ese valor\n",
    "        columnas_transpuestas = zip(*array_multidimensional)\n",
    "        nuevo_array = [column.index(max(column)) for column in columnas_transpuestas]\n",
    "\n",
    "        return nuevo_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0354a17c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'rate_decay': False, 'batch_tam': 32, 'normalizacion': False, 'rate': 0.1}\n",
      "Mejor rendimiento: 1.0\n",
      "Mejores parámetros: {'rate_decay': False, 'batch_tam': 32, 'normalizacion': False, 'rate': 0.1}\n",
      "Mejor rendimiento: 0.7642857142857142\n",
      "Mejores parámetros: {'rate_decay': False, 'batch_tam': 32, 'normalizacion': False, 'rate': 0.1}\n",
      "Mejor rendimiento: 0.9803921568627451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9583333333333334"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xe_iris,Xp_iris,ye_iris,yp_iris = particion_entr_prueba(X_iris,y_iris,test=1/3)\n",
    "\n",
    "rl_iris=RegresionLogisticaOvR()\n",
    "\n",
    "rl_iris.entrena(Xe_iris,ye_iris)\n",
    "\n",
    "#rendimiento(rl_iris,Xe_iris,ye_iris)\n",
    "# 0.9797979797979798\n",
    "\n",
    "rendimiento(rl_iris,Xp_iris,yp_iris)\n",
    "# >>> 0.9607843137254902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0977db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# EJERCICIO 6: APLICACION A PROBLEMAS MULTICLASE\n",
    "# ==============================================\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 6.1) Conjunto de datos de la concesión de crédito\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# Aplicar la implementación del apartado anterior, para obtener un\n",
    "# clasificador que aconseje la concesión, estudio o no concesión de un préstamo,\n",
    "# basado en los datos X_credito, y_credito. Ajustar adecuadamente los parámetros. \n",
    "\n",
    "# NOTA IMPORTANTE: En este caso concreto, los datos han de ser transformados, \n",
    "# ya que los atributos de este conjunto de datos no son numéricos. Para ello, usar la llamada \n",
    "# \"codificación one-hot\", descrita en el tema \"Preprocesado e ingeniería de características\".\n",
    "# Se pide implementar esta transformación (directamete, SIN USAR Scikt Learn ni Pandas). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ceb07fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(data):\n",
    "    # Inicializar una lista para almacenar los datos codificados One-Hot\n",
    "    encoded_data = []\n",
    "\n",
    "    # Iterar sobre cada columna de las 6 caracteristicas\n",
    "    for i in range(data.shape[1]):\n",
    "        # para cada categoria\n",
    "        columna_actual = data[:, i]\n",
    "        # calculamos los valores unicos\n",
    "        valores_unicos = np.unique(columna_actual)\n",
    "\n",
    "        # Iteramos sobre cada valor único y crear una nueva columna codificada\n",
    "        for valor_unico in valores_unicos:\n",
    "            # comprobamos y añadimos el valor correspondiente a la nueva columna\n",
    "            nueva_columna = (columna_actual == valor_unico).astype(int)\n",
    "            # añadimos la nueva columna\n",
    "            encoded_data.append(nueva_columna)\n",
    "\n",
    "    # Combinar las columnas codificadas ,recordemos que tenemos que transponer la matriz para que las caracteristicas esten ordenadas \n",
    "    encoded_data = np.array(encoded_data).T\n",
    "\n",
    "    return encoded_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459ef641",
   "metadata": {},
   "source": [
    "Los valores de los datos transformamos quedarían de la siguiente manera, hemos pasado de 6 caracteristicas a 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07247d68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(650, 20)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_oh =one_hot_encoding(X_credito)\n",
    "X_oh.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9bb9e",
   "metadata": {},
   "source": [
    "Los valores de clasificación del conjunto de concesiones de prestamo queda de la siguiente manera, podriamos usar como use en el apartado 3 un mapeo para cada uno de los 3 valores pero esta opción es mas generalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "97febe0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtenemos los valores únicos\n",
    "y_coded = np.unique(y_credito, return_inverse=True)[1]\n",
    "\n",
    "# Convertimos las etiquetas numéricas a un array\n",
    "y_recoded = np.array(y_coded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a39cc",
   "metadata": {},
   "source": [
    "Una vez tenemos el conjunto preparado llamamos a 0vR y calculamos el rendimiento de nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1d42d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "Xe_credito,Xp_credito,ye_credito,yp_credito = particion_entr_prueba(X_oh,y_recoded,test=1/3)\n",
    "\n",
    "rl_credito=RegresionLogisticaOvR(rate=0.001,batch_tam=20)\n",
    "\n",
    "rl_credito.entrena(Xe_credito,ye_credito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cca193f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7685185185185185"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rendimiento(rl_credito,Xp_credito,yp_credito)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a5ed935",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# 6.2) Clasificación de imágenes de dígitos escritos a mano\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "#  Aplicar la implementación o implementaciones del apartado anterior, para obtener un\n",
    "#  clasificador que prediga el dígito que se ha escrito a mano y que se\n",
    "#  dispone en forma de imagen pixelada, a partir de los datos que están en el\n",
    "#  archivo digidata.zip que se suministra.  Cada imagen viene dada por 28x28\n",
    "#  píxeles, y cada pixel vendrá representado por un caracter \"espacio en\n",
    "#  blanco\" (pixel blanco) o los caracteres \"+\" (borde del dígito) o \"#\"\n",
    "#  (interior del dígito). En nuestro caso trataremos ambos como un pixel negro\n",
    "#  (es decir, no distinguiremos entre el borde y el interior). En cada\n",
    "#  conjunto las imágenes vienen todas seguidas en un fichero de texto, y las\n",
    "#  clasificaciones de cada imagen (es decir, el número que representan) vienen\n",
    "#  en un fichero aparte, en el mismo orden. Será necesario, por tanto, definir\n",
    "#  funciones python que lean esos ficheros y obtengan los datos en el mismo\n",
    "#  formato numpy en el que los necesita el clasificador. \n",
    "\n",
    "#  Los datos están ya separados en entrenamiento, validación y prueba. En este\n",
    "#  caso concreto, NO USAR VALIDACIÓN CRUZADA para ajustar, ya que podría\n",
    "#  tardar bastante (basta con ajustar comparando el rendimiento en\n",
    "#  validación). Si el tiempo de cómputo en el entrenamiento no permite\n",
    "#  terminar en un tiempo razonable, usar menos ejemplos de cada conjunto.\n",
    "\n",
    "# Ajustar los parámetros de tamaño de batch, tasa de aprendizaje y\n",
    "# rate_decay para tratar de obtener un rendimiento aceptable (por encima del\n",
    "# 75% de aciertos sobre test). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "64315a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_digits(filename):\n",
    "    \n",
    "    # Aqui guardaremos la solucion\n",
    "    digits = []\n",
    "    # abrimos y leemos el archivo y sus lineas\n",
    "    with open(os.getcwd() + filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Recorremos las lineas\n",
    "    for line in lines:\n",
    "        \n",
    "        line_mod = ''.join(line)\n",
    "        # Aqui cambiamos el \"fondo\" por un caracter\n",
    "        line_mod = line_mod.replace(' ', '0')\n",
    "        # Convertimos en los + y # en un unico valor, ya que no nos interesa en etse caso diferenciar bordes del interior\n",
    "        line_mod = line_mod.replace('#', '1').replace('+', '1')\n",
    "        # Necesitamos eliminar los saltos de linea también, ya que sino da error, los cuales están al final\n",
    "        line_mod = line_mod.rstrip('\\n')\n",
    "\n",
    "\n",
    "\n",
    "        d = []\n",
    "        for digito in line_mod:\n",
    "            d.append(int(digito))\n",
    "        digits.append(d)\n",
    "\n",
    "        \n",
    "    digits = np.array(digits)\n",
    "    \n",
    "    #El valor de -1 indica que encuentre automaticamente el tamaño si es posible(en nuestro caso es 28x28=784)\n",
    "    digits = digits.reshape((int(digits.shape[0]/28),-1))\n",
    " \n",
    "    return digits\n",
    "\n",
    "\n",
    "X_test_digits = load_digits('/datos/digitdata/testimages')\n",
    "X_train_digits = load_digits('/datos/digitdata/trainingimages')\n",
    "X_validation_digits = load_digits('/datos/digitdata/validationimages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7292ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_valores(filename):\n",
    "    \n",
    "    #Aqui guardaremos los valores\n",
    "    valores = []\n",
    "\n",
    "    # Abrimos el archivo en la ruta y leemos el archivo\n",
    "    with open(os.getcwd() + filename, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        \n",
    "\n",
    "    # Por cada linea guardo el valor de clasificacion\n",
    "    for line in lines:\n",
    "        valores.append(int(line))\n",
    "\n",
    "    # Devuelvo un array\n",
    "    return np.array(valores)\n",
    "\n",
    "#Cargamos los 3 conjuntos\n",
    "y_test_valores = load_valores('/datos/digitdata/testlabels')\n",
    "y_train_valores = load_valores('/datos/digitdata/traininglabels')\n",
    "y_validation_valores = load_valores('/datos/digitdata/validationlabels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c5806a",
   "metadata": {},
   "source": [
    "Para la validación me quedo con 500 digitos y su clasificacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "350e2a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_digits=X_validation_digits[:500]\n",
    "y_validation_valores=y_validation_valores[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632949a3",
   "metadata": {},
   "source": [
    "para que no tarde tanto en entrenar pongo un único caso de params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f1727291",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = [\n",
    "    {\"rate_decay\": False, \"batch_tam\": 32, \"normalizacion\": False, \"rate\": 0.1},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "604c7f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "rl_digits=RegresionLogisticaOvR()\n",
    "\n",
    "rl_digits.entrena(X_validation_digits,y_validation_valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c56a10a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El rendimiento es : 79.0 %\n"
     ]
    }
   ],
   "source": [
    "score = rendimiento(rl_digits,X_test_digits,y_test_valores)\n",
    "print(\"El rendimiento es :\",score*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfabed3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a357f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05a839a1",
   "metadata": {},
   "source": [
    "# Bilbiografia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b041e8c1",
   "metadata": {},
   "source": [
    "https://medium.com/@heramb1008/gradient-descent-implementation-using-python-bc7d1df4ba88\n",
    "\n",
    "https://www.youtube.com/watch?v=YDa3rX9yLCE\n",
    "\n",
    "https://medium.com/@agrawalsam1997/multiclass-classification-onevsrest-and-onevsone-classification-strategy-2c293a91571a\n",
    "\n",
    "https://www.baeldung.com/cs/gradient-descent-vs-ascent\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
