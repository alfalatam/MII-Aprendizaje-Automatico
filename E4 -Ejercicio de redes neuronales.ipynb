{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes neuronales (ejercicio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importante: comentar adecuadamente cada paso realizado**, relacionándolo con lo visto en la teoría."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 1: aplicación de redes neuronales a clasificación (análisis de sentimientos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pide aplicar un modelo de redes neuronales al problema de decidir si una crítica de cine es positiva o negativa. Para ello volvemos a usar los datos de IMDB (Internet Movie Database) que vimos en el módulo 2 (modelo probabilístico).\n",
    "\n",
    "Hacerlo usando los dos sistemas vistos, comparando los resultados:\n",
    "* Scikit learn: usar `MLPClassifier`\n",
    "* Keras con Tensorflow: usar `Sequential` y capas tipo `Dense` con la arquitectura adecuda.\n",
    "\n",
    "\n",
    "Aunque ya hemos visto que los datos están disponibles en http://ai.stanford.edu/~amaas/data/sentiment/ , en este caso pedimos cargar los datos usando la utilidad `imdb`de Keras. Se puede consultar en el manual de Keras: https://keras.io/datasets/ Cargarlos con `imdb.load_data` y usar los datos cargados como punto de partida a este ejercicio (tanto para su aplicar scikit learn como para aplicar keras). Prestar atención al formato en el que se cargar, que no es el mismo que hamos visto hasta ahora.  \n",
    "\n",
    "Los textos han de ser vectorizados para que se puedan ser procesados por una red. Para esto, tenemos varias alternativas, usar una de ellas:\n",
    "\n",
    "* Vectorizando \"manualmente\", definiendo una función en python que lo haga.\n",
    "* Vectorizadores de scikit learn (ya vistos)\n",
    "* Herramientas de vectorización de keras: https://keras.io/preprocessing/text/\n",
    "\n",
    "Mostrar algunas pruebas realizadas con distintas arquitecturas y/o hiperparámetros. No es necesario ser muy exhaustivo ni usar `GridSearchCV` en scikit learn ni el equivalente en keras. Tan solo mostrar alguna experimentación y ajuste manual.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alarc\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Lo primero que tenemos que hacer como dice el enunciado es cargar el conjunto de datos y vectorizarlo, para esto he decidido usar la 3º opción (keras) y especificando un límite de palabras a 12000 ya que mi portatil no puede con tanto cálculo.\n",
    "\n",
    "con keras he llamado a Tokenizer y con su método sequencestomatrix que me permite convetir la lista en matrices que contienen  la frecuencia de cada palabra, es decir, las matrices x_train y x_test_keras contienen una matriz donde me indica la frecuencia de cada palabra gracias al mode=count (con un mode=binary obtendriamos un valor de si esa palabra esta o no)\n",
    "\n",
    "Después aplicamos redes neuronales usando MLPClassifier, le paso el número de capas internas, que en este primer caso serán dos de 16 cada una y límito el número de iteraciones a 1000, entrenamos el modelo y calculamos el rendimiento\n",
    "\n",
    "Como dice el enunciado he probado a usar un número distinto de capas o la densidad de esta y en el tercer caso con una regularización distinta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 12000\n",
    "\n",
    "imdb = keras.datasets.imdb.load_data(num_words=max_words)\n",
    "(X_train, y_train), (X_test, y_test) = imdb\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "x_train_keras = tokenizer.sequences_to_matrix(X_train, mode='count')\n",
    "x_test_keras = tokenizer.sequences_to_matrix(X_test, mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento con Scikit-learn: 0.86\n"
     ]
    }
   ],
   "source": [
    "# Primero probamos con 2 capas internas de 16 y comprobamos el rendimiento tras entrenar el modelo\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(16, 16), max_iter=1000, random_state=42)\n",
    "mlp_model.fit(x_train_keras, y_train)\n",
    "score = mlp_model.score(x_test_keras, y_test)\n",
    "\n",
    "print('Rendimiento con Scikit-learn: {}'.format(round(score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento con Scikit-learn: 0.859\n"
     ]
    }
   ],
   "source": [
    "# Eliminamos una capa y comprobamos si el rendimiento mejora\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(16), max_iter=1000, random_state=42)\n",
    "mlp_model.fit(x_train_keras, y_train)\n",
    "score = mlp_model.score(x_test_keras, y_test)\n",
    "\n",
    "print('Rendimiento con Scikit-learn: {}'.format(round(score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento con Scikit-learn: 0.854\n"
     ]
    }
   ],
   "source": [
    "# Como no ha mejorado vamos a probar con una tercera capa y cambiamos la regularización a 0.1\n",
    "\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(32, 16 , 8),alpha=0.1, max_iter=1000, random_state=42)\n",
    "mlp_model.fit(x_train_keras, y_train)\n",
    "score = mlp_model.score(x_test_keras, y_test)\n",
    "\n",
    "print('Rendimiento con Scikit-learn: {}'.format(round(score, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer lo mismo pero usando Keras, hemos empleado capas tipo Dense para las intermedias y una función de activación relu.\n",
    "\n",
    "Para la última capa al ser un problema de clasificación binaria usaremos como función de activación la sigmoide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alarc\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keras_model = Sequential([\n",
    "    Flatten(input_shape=(max_words,)),\n",
    "    Dense(300, activation='relu'),\n",
    "    Dense(100, activation='relu'),\n",
    "    Dense(1, activation='sigmoid') \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\alarc\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:From C:\\Users\\alarc\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\alarc\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "40/40 [==============================] - 3s 46ms/step - loss: 0.6694 - accuracy: 0.6030 - val_loss: 0.6550 - val_accuracy: 0.6274\n",
      "Epoch 2/3\n",
      "40/40 [==============================] - 1s 34ms/step - loss: 0.6290 - accuracy: 0.6714 - val_loss: 0.6137 - val_accuracy: 0.6844\n",
      "Epoch 3/3\n",
      "40/40 [==============================] - 1s 34ms/step - loss: 0.6149 - accuracy: 0.6896 - val_loss: 0.5821 - val_accuracy: 0.7224\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.5919 - accuracy: 0.7175\n",
      "Rendimiento con keras: 0.718\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo de Keras y comprobación del rendimiento con un optimizador y un número de epochs específico\n",
    "keras_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "keras_model.fit(x_train_keras, y_train, epochs=3, batch_size=512, validation_split=0.2)\n",
    "keras_loss, keras_accuracy = keras_model.evaluate(x_test_keras, y_test)\n",
    "print('Rendimiento con keras: {}'.format(round(keras_accuracy, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "40/40 [==============================] - 5s 73ms/step - loss: 0.4301 - accuracy: 0.8140 - val_loss: 0.2947 - val_accuracy: 0.8902\n",
      "Epoch 2/3\n",
      "40/40 [==============================] - 2s 58ms/step - loss: 0.1814 - accuracy: 0.9340 - val_loss: 0.2827 - val_accuracy: 0.8960\n",
      "Epoch 3/3\n",
      "40/40 [==============================] - 2s 56ms/step - loss: 0.0785 - accuracy: 0.9751 - val_loss: 0.3408 - val_accuracy: 0.8932\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.3846 - accuracy: 0.8731\n",
      "Rendimiento con keras: 0.873\n"
     ]
    }
   ],
   "source": [
    "# Como el rendimiento es malo, vamos a evaluar el modelo de Keras usando adam como el optimizador\n",
    "keras_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "keras_model.fit(x_train_keras, y_train, epochs=3, batch_size=512, validation_split=0.2)\n",
    "keras_loss, keras_accuracy = keras_model.evaluate(x_test_keras, y_test)\n",
    "print('Rendimiento con keras: {}'.format(round(keras_accuracy, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "40/40 [==============================] - 4s 69ms/step - loss: 0.0575 - accuracy: 0.9813 - val_loss: 0.4191 - val_accuracy: 0.8888\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 2s 57ms/step - loss: 0.0117 - accuracy: 0.9979 - val_loss: 0.4807 - val_accuracy: 0.8848\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 2s 53ms/step - loss: 0.0083 - accuracy: 0.9987 - val_loss: 0.5426 - val_accuracy: 0.8858\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 0.0022 - accuracy: 0.9998 - val_loss: 0.6066 - val_accuracy: 0.8852\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 7.4072e-04 - accuracy: 1.0000 - val_loss: 0.6570 - val_accuracy: 0.8846\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 2s 52ms/step - loss: 3.1861e-04 - accuracy: 1.0000 - val_loss: 0.6973 - val_accuracy: 0.8858\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 2s 54ms/step - loss: 1.7847e-04 - accuracy: 1.0000 - val_loss: 0.7354 - val_accuracy: 0.8872\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 2s 57ms/step - loss: 1.0865e-04 - accuracy: 1.0000 - val_loss: 0.7710 - val_accuracy: 0.8858\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 2s 55ms/step - loss: 7.0146e-05 - accuracy: 1.0000 - val_loss: 0.8020 - val_accuracy: 0.8882\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 2s 56ms/step - loss: 4.8091e-05 - accuracy: 1.0000 - val_loss: 0.8295 - val_accuracy: 0.8868\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.9027 - accuracy: 0.8699\n",
      "Rendimiento con keras: 0.87\n"
     ]
    }
   ],
   "source": [
    "# En este tercer vamos a probar con adam ya que parece que da mejor resultado pero aumentando los epochs \n",
    "keras_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "keras_model.fit(x_train_keras, y_train, epochs=10, batch_size=512, validation_split=0.2)\n",
    "keras_loss, keras_accuracy = keras_model.evaluate(x_test_keras, y_test)\n",
    "print('Rendimiento con keras: {}'.format(round(keras_accuracy, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parte 2: aplicación de redes neuronales a regresión (predicción del precio de la vivienda)\n",
    "\n",
    "Se pide aplicar un modelo de redes neuronales al problema de predecir precios de vivienda usando el conjunto de datos  `Boston house prices`. \n",
    "\n",
    "Hacerlo usando los dos sistemas vistos, comparando los resultados:\n",
    "* Scikit learn: usar `MLPRegressor`\n",
    "* Keras con Tensorflow: usar nuevamente `Sequential` y capas tipo `Dense` con la arquitectura adecuada.\n",
    "\n",
    "El conjunto de datos se puede cargar usando tanto scikit learn (`sklearn.datasets.load_boston`) como keras (`keras.datasets.boston_housing`). \n",
    "\n",
    "Como en la parte 1, se pide mostrar algunas pruebas de los resultados obtenidos usando distintas arquitecturas y/o hiperparámetros. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero que vamos a hacer aqui es cargar y normalizar los valores del conjunto de boston, ya que como hemos visto en teoría son muy sensibles a las diferencias de magnitud de las unidades\n",
    "\n",
    "Y crearemos el conjunto de entrenamiento (load_boston está ya desactualizado, debido a eso me sale un warning avisando).\n",
    "\n",
    "Después simplemente probaremos variaciones para comprobar coomo fluctua el rendimiento para los distintos valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alarc\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# Cargarmos boston\n",
    "boston = load_boston()\n",
    "x, y = boston.data, boston.target\n",
    "\n",
    "# Normalizar los datos para las redes\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(x)\n",
    "\n",
    "# dividimos los datos\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento con Scikit-learn MLPRegressor: 0.729\n"
     ]
    }
   ],
   "source": [
    "# Modelo MLPRegressor de Scikit-learn con dos capas internas\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(50, 30), max_iter=500, random_state=42)\n",
    "mlp_regressor.fit(x_train, y_train)\n",
    "score = mlp_regressor.score(x_test, y_test)\n",
    "\n",
    "print('Rendimiento con Scikit-learn MLPRegressor: {}'.format(round(score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento con Scikit-learn MLPRegressor: 0.807\n"
     ]
    }
   ],
   "source": [
    "# Modelo MLPRegressor de Scikit-learn con una única capa de 50 y aumentando el nº máxmo de iteraciones\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(50), max_iter=1000, random_state=42)\n",
    "mlp_regressor.fit(x_train, y_train)\n",
    "score = mlp_regressor.score(x_test, y_test)\n",
    "\n",
    "print('Rendimiento con Scikit-learn MLPRegressor: {}'.format(round(score, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendimiento con Scikit-learn MLPRegressor: 0.824\n"
     ]
    }
   ],
   "source": [
    "# Modelo MLPRegressor de Scikit-learn añadiendo una tercera capa\n",
    "mlp_regressor = MLPRegressor(hidden_layer_sizes=(50, 50, 20), max_iter=1000, random_state=42)\n",
    "mlp_regressor.fit(x_train, y_train)\n",
    "score = mlp_regressor.score(x_test, y_test)\n",
    "\n",
    "print('Rendimiento con Scikit-learn MLPRegressor: {}'.format(round(score, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por último vamos a aplicar keras sobre este conjunto, vamos a empezar con dos capas intermedias que luego variaremos para comprobar como se comporta el método y de salida una capa sin función de activación(lineal por defecto) ya que nos encontramos ante un problema de regresión\n",
    "\n",
    "La función compile permite especificarle una serie de parámetros tales como un optimizador encargado de calcular los pesos óptimos o una función de pérdida ya que estamos en un problema de regresión\n",
    "    \n",
    "\n",
    "Finalmente entrenamos el modelo y evaluamos comprobando la pérdida en cada uno de los casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 1s 18ms/step - loss: 624.1054 - mean_squared_error: 624.1054 - val_loss: 542.5886 - val_mean_squared_error: 542.5886\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 586.2067 - mean_squared_error: 586.2067 - val_loss: 509.5023 - val_mean_squared_error: 509.5023\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 547.7231 - mean_squared_error: 547.7231 - val_loss: 471.7669 - val_mean_squared_error: 471.7669\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 501.5439 - mean_squared_error: 501.5439 - val_loss: 425.5523 - val_mean_squared_error: 425.5523\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 444.8545 - mean_squared_error: 444.8545 - val_loss: 369.1151 - val_mean_squared_error: 369.1151\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 377.7365 - mean_squared_error: 377.7365 - val_loss: 304.5733 - val_mean_squared_error: 304.5733\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 303.1766 - mean_squared_error: 303.1766 - val_loss: 234.2500 - val_mean_squared_error: 234.2500\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 226.8506 - mean_squared_error: 226.8506 - val_loss: 168.4104 - val_mean_squared_error: 168.4104\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 162.7834 - mean_squared_error: 162.7834 - val_loss: 112.7876 - val_mean_squared_error: 112.7876\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 113.5908 - mean_squared_error: 113.5908 - val_loss: 77.9631 - val_mean_squared_error: 77.9631\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dccb6c0890>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo Sequential de Keras\n",
    "keras_model_reg = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # No se utiliza función de activación(lineal por defecto) en la capa de salida para regresión\n",
    "])\n",
    "\n",
    "# Compilamos el modelo calculando el error cuadratico para ver el rendimiento\n",
    "keras_model_reg.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# Entrenamos el modelo de Keras\n",
    "keras_model_reg.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 1ms/step - loss: 74.6138 - mean_squared_error: 74.6138\n"
     ]
    }
   ],
   "source": [
    "# Calculamos la pérdida para el modelo de Keras\n",
    "keras_score = keras_model_reg.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "11/11 [==============================] - 1s 17ms/step - loss: 632.9772 - mean_squared_error: 632.9772 - val_loss: 554.3345 - val_mean_squared_error: 554.3345\n",
      "Epoch 2/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 605.3443 - mean_squared_error: 605.3443 - val_loss: 538.2869 - val_mean_squared_error: 538.2869\n",
      "Epoch 3/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 585.3171 - mean_squared_error: 585.3171 - val_loss: 514.5273 - val_mean_squared_error: 514.5273\n",
      "Epoch 4/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 553.1409 - mean_squared_error: 553.1409 - val_loss: 477.8574 - val_mean_squared_error: 477.8574\n",
      "Epoch 5/10\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 505.9354 - mean_squared_error: 505.9354 - val_loss: 425.5505 - val_mean_squared_error: 425.5505\n",
      "Epoch 6/10\n",
      "11/11 [==============================] - 0s 6ms/step - loss: 439.4791 - mean_squared_error: 439.4791 - val_loss: 354.0451 - val_mean_squared_error: 354.0451\n",
      "Epoch 7/10\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 349.3675 - mean_squared_error: 349.3675 - val_loss: 257.5511 - val_mean_squared_error: 257.5511\n",
      "Epoch 8/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 242.5711 - mean_squared_error: 242.5711 - val_loss: 153.4863 - val_mean_squared_error: 153.4863\n",
      "Epoch 9/10\n",
      "11/11 [==============================] - 0s 5ms/step - loss: 140.8045 - mean_squared_error: 140.8045 - val_loss: 84.4457 - val_mean_squared_error: 84.4457\n",
      "Epoch 10/10\n",
      "11/11 [==============================] - 0s 4ms/step - loss: 88.3230 - mean_squared_error: 88.3230 - val_loss: 53.6898 - val_mean_squared_error: 53.6898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1dcbdd0a610>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vamos a probar ahora otro caso con una capa intermedia adicional \n",
    "\n",
    "keras_model_reg = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(x_train.shape[1],)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(1)  # No se utiliza función de activación(lineal por defecto) en la capa de salida para regresión\n",
    "])\n",
    "\n",
    "# Compilamos el modelo calculando el error cuadratico para ver el rendimiento\n",
    "keras_model_reg.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "\n",
    "# Entrenamos el modelo de Keras\n",
    "keras_model_reg.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 0s/step - loss: 58.2393 - mean_squared_error: 58.2393\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el rendimiento en el modelo de Keras y podemos comprobar que en este segundo caso la pérdida es menor\n",
    "keras_score = keras_model_reg.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
